{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bay Wheels's trip Dataset Exploration\n",
    "## by Chrysanthi Polyzoni\n",
    "\n",
    "> We will be exploring data teken from [Bay Wheels Bike share service](https://www.lyft.com/bikes/bay-wheels/system-data). Bay Wheels offers affordable, accessible, and fun transportation option for everyone. [Bay Area](https://it.wikipedia.org/wiki/San_Francisco_Bay_Area) residents who qualify for CalFresh, SFMTA Lifeline Pass, or PG&E CARE utility discount are eligible to join Bike Share for All program for 5 USD for the first year — now accepting prepaid cards!\n",
    "\n",
    "The Data\n",
    "Each trip is anonymized and includes:\n",
    "\n",
    "* Trip Duration (seconds)\n",
    "* Start Time and Date\n",
    "* End Time and Date\n",
    "* Start Station ID\n",
    "* Start Station Name\n",
    "* Start Station Latitude\n",
    "* Start Station Longitude\n",
    "* End Station ID\n",
    "* End Station Name\n",
    "* End Station Latitude\n",
    "* End Station Longitude\n",
    "* Bike ID\n",
    "* User Type (Subscriber or Customer – “Subscriber” = Member or “Customer” = Casual\n",
    "\n",
    "This data is provided according to the [Bay Wheels License Agreement](https://baywheels-assets.s3.amazonaws.com/data-license-agreement.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "## Preliminary Wrangling\n",
    "\n",
    "- [Gathering the Data](#aquire)\n",
    "- [Assessing](#assess)\n",
    "- [Cleaning](#clean)\n",
    "- [Exploratory Data Analysis](#analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aquire'></a>\n",
    "## Gathering the Data for the greater San Fransisco Bay Area:\n",
    "The Data for one month of a year is saved in a zipped 'csv' File on the [Website of the Company](https://s3.amazonaws.com/baywheels-data/index.html) . As an example, the File '201801-fordgobike-tripdata.csv.zip' contains all the rides that occured in January 2018.\n",
    "After programmatically downloading and unzipping all available months of the year 2018,\n",
    "We will load and concatenate all <i>CSV</i> Files in a Dataframe where we will be conducting our analysis on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages and set plots to be embedded inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load in your dataset and describe its properties through the questions below.\n",
    "Try and motivate your exploration goals through this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading all necessary Files for the year 2019 programmatically:¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make ./data\n",
      "./data\\201905-baywheels-tripdata.csv.zip does not exist. --> download it\n",
      "./data\\201905-baywheels-tripdata.csv.zip --> unzipping file\n",
      "./data\\201905-baywheels-tripdata.csv.zip--> removing file\n",
      "./data\\201906-baywheels-tripdata.csv.zip does not exist. --> download it\n",
      "./data\\201906-baywheels-tripdata.csv.zip --> unzipping file\n",
      "./data\\201906-baywheels-tripdata.csv.zip--> removing file\n",
      "./data\\201907-baywheels-tripdata.csv.zip does not exist. --> download it\n",
      "./data\\201907-baywheels-tripdata.csv.zip --> unzipping file\n",
      "./data\\201907-baywheels-tripdata.csv.zip--> removing file\n",
      "./data\\201908-baywheels-tripdata.csv.zip does not exist. --> download it\n",
      "./data\\201908-baywheels-tripdata.csv.zip --> unzipping file\n",
      "./data\\201908-baywheels-tripdata.csv.zip--> removing file\n",
      "./data\\201909-baywheels-tripdata.csv.zip does not exist. --> download it\n",
      "./data\\201909-baywheels-tripdata.csv.zip --> unzipping file\n",
      "./data\\201909-baywheels-tripdata.csv.zip--> removing file\n",
      "./data\\201910-baywheels-tripdata.csv.zip does not exist. --> download it\n",
      "./data\\201910-baywheels-tripdata.csv.zip --> unzipping file\n",
      "./data\\201910-baywheels-tripdata.csv.zip--> removing file\n",
      "./data\\201911-baywheels-tripdata.csv.zip does not exist. --> download it\n",
      "./data\\201911-baywheels-tripdata.csv.zip --> unzipping file\n",
      "./data\\201911-baywheels-tripdata.csv.zip--> removing file\n",
      "./data\\201912-baywheels-tripdata.csv.zip does not exist. --> download it\n",
      "./data\\201912-baywheels-tripdata.csv.zip --> unzipping file\n",
      "./data\\201912-baywheels-tripdata.csv.zip--> removing file\n",
      "./data\\201901-fordgobike-tripdata.csv.zip does not exist. --> download it\n",
      "./data\\201901-fordgobike-tripdata.csv.zip --> unzipping file\n",
      "./data\\201901-fordgobike-tripdata.csv.zip--> removing file\n",
      "./data\\201902-fordgobike-tripdata.csv.zip does not exist. --> download it\n",
      "./data\\201902-fordgobike-tripdata.csv.zip --> unzipping file\n",
      "./data\\201902-fordgobike-tripdata.csv.zip--> removing file\n",
      "./data\\201903-fordgobike-tripdata.csv.zip does not exist. --> download it\n",
      "./data\\201903-fordgobike-tripdata.csv.zip --> unzipping file\n",
      "./data\\201903-fordgobike-tripdata.csv.zip--> removing file\n",
      "./data\\201904-fordgobike-tripdata.csv.zip does not exist. --> download it\n",
      "./data\\201904-fordgobike-tripdata.csv.zip --> unzipping file\n",
      "./data\\201904-fordgobike-tripdata.csv.zip--> removing file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['201901-fordgobike-tripdata.csv',\n",
       " '201902-fordgobike-tripdata.csv',\n",
       " '201903-fordgobike-tripdata.csv',\n",
       " '201904-fordgobike-tripdata.csv',\n",
       " '201905-baywheels-tripdata.csv',\n",
       " '201906-baywheels-tripdata.csv',\n",
       " '201907-baywheels-tripdata.csv',\n",
       " '201908-baywheels-tripdata.csv',\n",
       " '201909-baywheels-tripdata.csv',\n",
       " '201910-baywheels-tripdata.csv',\n",
       " '201911-baywheels-tripdata.csv',\n",
       " '201912-baywheels-tripdata.csv',\n",
       " '__MACOSX']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "csv_downloaded = False # Set to True to prevent downloading Process from occuring \\\n",
    "                      #every time we re-run the notebook\n",
    "filename_ford = '2019xx-fordgobike-tripdata.csv.zip'\n",
    "filename_wheels = '2019xx-baywheels-tripdata.csv.zip'\n",
    "base_url = 'https://s3.amazonaws.com/baywheels-data/'\n",
    "\n",
    "datafolder = './data' # keeping all csv files in a 'data' directory, under the current directory\n",
    "if not os.path.exists(datafolder):\n",
    "    os.makedirs(datafolder)\n",
    "    print('make {}'.format(datafolder))\n",
    "\n",
    "for i in range(5, 13):\n",
    "    if csv_downloaded:\n",
    "        break\n",
    "    \n",
    "    iter_wheels = str(filename_wheels).replace(\"xx\", \"{:02d}\".format(i))\n",
    "    \n",
    "    url_wheels = base_url + iter_wheels\n",
    "    \n",
    "    local_wheels = os.path.join(datafolder, iter_wheels)\n",
    "    \n",
    "    # download the zip file locally\n",
    "    if not os.path.exists(local_wheels):\n",
    "        print(\"{} does not exist. --> download it\".format(local_wheels))\n",
    "        try:\n",
    "            response = requests.get(url_wheels)\n",
    "            with open(local_wheels, mode='wb') as file:\n",
    "                file.write(response.content)\n",
    "                    # unzip file to the previously created 'data' directory\n",
    "                with zipfile.ZipFile(local_wheels, 'r') as myzip:\n",
    "                    myzip.extractall(path = datafolder)\n",
    "                    print(\"{} --> unzipping file\".format(local_wheels))\n",
    "            os.remove(local_wheels)\n",
    "            print(\"{}--> removing initial zip file\".format(local_wheels))\n",
    "        except OSError as err:\n",
    "            print(\"OS error: {0}\".format(err))\n",
    "            \n",
    "for i in range(1, 5):\n",
    "    if csv_downloaded:\n",
    "        break\n",
    "    iter_ford = str(filename_ford).replace(\"xx\", \"{:02d}\".format(i))\n",
    "    url_ford = base_url + iter_ford\n",
    "    local_ford = os.path.join(datafolder, iter_ford)\n",
    "\n",
    "            \n",
    "    if not os.path.exists(local_ford):\n",
    "        print(\"{} does not exist. --> download it\".format(local_ford))\n",
    "        try:\n",
    "            response = requests.get(url_ford)\n",
    "            with open(local_ford, mode='wb') as file:\n",
    "                file.write(response.content)\n",
    "                    # unzip file to the previously created 'data' directory\n",
    "                with zipfile.ZipFile(local_ford, 'r') as myzip:\n",
    "                    myzip.extractall(path = datafolder)\n",
    "                    print(\"{} --> unzipping file\".format(local_ford))\n",
    "            os.remove(local_ford)\n",
    "            print(\"{}--> removing initial zip file\".format(local_ford))\n",
    "        except OSError as err:\n",
    "            print(\"OS error: {0}\".format(err))\n",
    "\n",
    "os.listdir(datafolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading 201901-fordgobike-tripdata.csv into df_01 Pandas Data Frame with dimensions (192082, 14)\n",
      "Updated shape after dropping non consistent features in all DataFrames is (192082, 13)\n",
      "\n",
      "Reading 201902-fordgobike-tripdata.csv into df_02 Pandas Data Frame with dimensions (183412, 14)\n",
      "Updated shape after dropping non consistent features in all DataFrames is (183412, 13)\n",
      "\n",
      "Reading 201903-fordgobike-tripdata.csv into df_03 Pandas Data Frame with dimensions (256299, 14)\n",
      "Updated shape after dropping non consistent features in all DataFrames is (256299, 13)\n",
      "\n",
      "Reading 201904-fordgobike-tripdata.csv into df_04 Pandas Data Frame with dimensions (239111, 14)\n",
      "Updated shape after dropping non consistent features in all DataFrames is (239111, 13)\n",
      "\n",
      "Reading 201905-baywheels-tripdata.csv into df_05 Pandas Data Frame with dimensions (182163, 14)\n",
      "Updated shape after dropping non consistent features in all DataFrames is (182163, 13)\n",
      "\n",
      "Reading 201906-baywheels-tripdata.csv into df_06 Pandas Data Frame with dimensions (191772, 15)\n",
      "Updated shape after dropping non consistent features in all DataFrames is (191772, 13)\n",
      "\n",
      "Reading 201907-baywheels-tripdata.csv into df_07 Pandas Data Frame with dimensions (258102, 15)\n",
      "Updated shape after dropping non consistent features in all DataFrames is (258102, 13)\n",
      "\n",
      "Reading 201908-baywheels-tripdata.csv into df_08 Pandas Data Frame with dimensions (210563, 14)\n",
      "Updated shape after dropping non consistent features in all DataFrames is (210563, 13)\n",
      "\n",
      "Reading 201909-baywheels-tripdata.csv into df_09 Pandas Data Frame with dimensions (217986, 14)\n",
      "Updated shape after dropping non consistent features in all DataFrames is (217986, 13)\n",
      "\n",
      "Reading 201910-baywheels-tripdata.csv into df_10 Pandas Data Frame with dimensions (239895, 14)\n",
      "Updated shape after dropping non consistent features in all DataFrames is (239895, 13)\n",
      "\n",
      "Reading 201911-baywheels-tripdata.csv into df_11 Pandas Data Frame with dimensions (185496, 15)\n",
      "Updated shape after dropping non consistent features in all DataFrames is (185496, 13)\n",
      "\n",
      "Reading 201912-baywheels-tripdata.csv into df_12 Pandas Data Frame with dimensions (150102, 14)\n",
      "Updated shape after dropping non consistent features in all DataFrames is (150102, 13)\n",
      "\n",
      "Number of dfs read:  12\n",
      "Number of rows read:  2506983\n",
      "Wall time: 9.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_list = [] # list of dataframes to read the csv files in.\n",
    "x= 0\n",
    "\n",
    "for file in os.listdir(datafolder):\n",
    "    if file.endswith('.csv'):\n",
    "        col_list = ['duration_sec', 'start_time', 'end_time', 'start_station_id', 'start_station_name', 'start_station_latitude', 'start_station_longitude', 'end_station_id', 'end_station_name', 'end_station_latitude', 'end_station_longitude', 'bike_id', 'user_type']\n",
    "        df_name = 'df_' + str(file[4:6])\n",
    "        df_temp = pd.read_csv('./data/' + file)\n",
    "        print('\\nReading {} into {} Pandas Data Frame with dimensions {}'.format(file, df_name, df_temp.shape))\n",
    "        df_features = list(df_temp.columns)\n",
    "        #print(df_features)\n",
    "        for col in df_temp.columns:\n",
    "            if col not in col_list:\n",
    "                df_temp.drop(col, axis=1, inplace = True)\n",
    "        print('Updated shape after dropping non consistent features in all DataFrames is {}'.format(df_temp.shape))\n",
    "        df_list.append(df_temp)\n",
    "        x += df_temp.shape[0]\n",
    "\n",
    "print('\\nNumber of dfs read: ', len(df_list))\n",
    "print('Number of rows read: ', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the structure of your dataset?\n",
    "\n",
    "> My dataset has 2 506 983 reccords (in other words attributes or lines) and 13 columns\n",
    "\n",
    "### What is/are the main feature(s) of interest in your dataset?\n",
    "\n",
    "> * Trip Duration (seconds) `duration_sec`\n",
    "* Start Time and Date `start_time`\n",
    "* End Time and Date `end_time`\n",
    "* Start Station ID `start_station_id`\n",
    "* Start Station Name `start_station_name`\n",
    "* Start Station Latitude `start_station_latitude`\n",
    "* Start Station Longitude `start_station_longitude`\n",
    "* End Station ID `end_station_id`\n",
    "* End Station Name `end_station_name`\n",
    "* End Station Latitude `end_station_latitude`\n",
    "* End Station Longitude `end_station_longitude`\n",
    "* Bike ID `bike_id`\n",
    "* User Type (Subscriber or Customer – “Subscriber” = Member or “Customer” = Casual `user_type`\n",
    "\n",
    "\n",
    "### What features in the dataset do you think will help support your investigation into your feature(s) of interest?\n",
    "\n",
    "> * Trip Duration (seconds) `duration_sec`\n",
    "* Start Time and Date `start_time`\n",
    "* End Time and Date `end_time`\n",
    "* Start Station ID `start_station_id`\n",
    "* Start Station Name `start_station_name`\n",
    "* Start Station Latitude `start_station_latitude`\n",
    "* Start Station Longitude `start_station_longitude`\n",
    "* End Station ID `end_station_id`\n",
    "* End Station Name `end_station_name`\n",
    "* End Station Latitude `end_station_latitude`\n",
    "* End Station Longitude `end_station_longitude`\n",
    "* Bike ID `bike_id`\n",
    "* User Type (Subscriber or Customer – “Subscriber” = Member or “Customer” = Casual `user_type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Exploration\n",
    "\n",
    "> In this section, investigate distributions of individual variables. If\n",
    "you see unusual points or outliers, take a deeper look to clean things up\n",
    "and prepare yourself to look at relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make sure that, after every plot or related series of plots, that you\n",
    "include a Markdown cell with comments about what you observed, and what\n",
    "you plan on investigating next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss the distribution(s) of your variable(s) of interest. Were there any unusual points? Did you need to perform any transformations?\n",
    "\n",
    "> Your answer here!\n",
    "\n",
    "### Of the features you investigated, were there any unusual distributions? Did you perform any operations on the data to tidy, adjust, or change the form of the data? If so, why did you do this?\n",
    "\n",
    "> Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Exploration\n",
    "\n",
    "> In this section, investigate relationships between pairs of variables in your\n",
    "data. Make sure the variables that you cover here have been introduced in some\n",
    "fashion in the previous section (univariate exploration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk about some of the relationships you observed in this part of the investigation. How did the feature(s) of interest vary with other features in the dataset?\n",
    "\n",
    "> Your answer here!\n",
    "\n",
    "### Did you observe any interesting relationships between the other features (not the main feature(s) of interest)?\n",
    "\n",
    "> Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Exploration\n",
    "\n",
    "> Create plots of three or more variables to investigate your data even\n",
    "further. Make sure that your investigations are justified, and follow from\n",
    "your work in the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk about some of the relationships you observed in this part of the investigation. Were there features that strengthened each other in terms of looking at your feature(s) of interest?\n",
    "\n",
    "> Your answer here!\n",
    "\n",
    "### Were there any interesting or surprising interactions between features?\n",
    "\n",
    "> Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At the end of your report, make sure that you export the notebook as an\n",
    "html file from the `File > Download as... > HTML` menu. Make sure you keep\n",
    "track of where the exported file goes, so you can put it in the same folder\n",
    "as this notebook for project submission. Also, make sure you remove all of\n",
    "the quote-formatted guide notes like this one before you finish your report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
